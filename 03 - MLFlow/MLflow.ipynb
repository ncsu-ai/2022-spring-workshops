{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 03 - MLflow\n",
    "\n",
    "#### Table of Contents\n",
    "- [Introduction](#introduction)\n",
    "- [Machine Learning Lifecycle](#machine-learning-lifecycle)\n",
    "- [Provided Files](#provided-files)\n",
    "- [Using MLflow](#using-mlflow)\n",
    "\n",
    "MLflow is an open-source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment and a central model registry, and it currently offers four components:\n",
    "<div style=\"width: 100%;\"><img src=\"img/mlflow_components.jpg\"/></div>\n",
    "\n",
    "- **MLflow Tracking --** keeps track of runs by saving metrics, parameters, tags and artifacts. It allows us to visualize and compare them in a browser in a user-friendly manner. Also, it creates different files with the description of the environment in which the run was executed (MLmodel, conda.yaml, model code).\n",
    "- **MLflow Project --** is a format for packaging data science code in a reusable and reproducible way. It uses artifacts recorded at the tracking step.\n",
    "- **MLflow Model --** is a standard format for packaging the models. The format defines a convention that lets you save a model in different flavors (e.g. Python function, R function, Scikit-learn, TensorFlow, Spark MLlib…) that can be understood by different downstream tools.\n",
    "- **MLflow Registry --** is a centralized model store. It provides model lineage (which run produced the model), model versioning, stage transitions (for example from staging to production) and annotations.\n",
    "\n",
    "## Machine Learning Lifecycle\n",
    "Before starting to work with MLflow and PyTorch, we will first discuss a high level overview of the machine learning lifecycle.\n",
    "<div style=\"width: 100%;\"><img src=\"img/ml_lifecycle.jpg\"/></div>\n",
    "\n",
    "- **Business Problem --** This is the first step of the machine learning workflow. It may take from few days to a few weeks to complete depending on the use case and complexity of the problem. It is at this stage that data scientists meet with subject-matter experts (SME’s) to gain an understanding of the problem, interview key stakeholders, collect information, and set the overall expectations of the project.\n",
    "- **Data Sourcing & ETL --** Once a detailed understanding of the business problem is achieved, it then comes to using the information gained during interviews to source the correct data for training your model(s), generally from an enterprise database.\n",
    "- **Exploratory Data Analysis (EDA) --** EDA is where you analyze the raw data. Your goal is to explore and assess the quality of the data, find missing values, feature distribution, correlation, etc.\n",
    "- **Data Preparation --** Now it is time to prepare the data for model training. This includes things like dividing the data into training and testing sets, feature encoding (e.g., one-hot-encoding, target encoding), feature engineering and selection, etc.\n",
    "- **Model Training & Selection --** This is the step everyone is excited about. This involves training a variety of models, tuning hyperparameters, model ensembling, evaluating performance metrics, model analysis (e.g., AUC, confusion matrix, residuals), and finally selecting one best model to be deployed in production for business use.\n",
    "- **Deployment & Monitoring --** This is the final step which is mostly concerned with MLOps. This includes things like packaging your final model, creating a docker image, writing a scoring script, and then making it all work together. Finally, you publish it as an API that can be used to obtain predictions on new data coming through your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Provided Files\n",
    "\n",
    "`MLproject`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "name: MNIST Classifier\n",
    "\n",
    "entry_points:\n",
    "  main:\n",
    "    parameters:\n",
    "      batch_size_train: {type: int, default: 64}\n",
    "      batch_size_test: {type: int, default: 10000}\n",
    "      n_epochs: {type: int, default: 3}\n",
    "      learning_rate: {type: float, default: 0.01}\n",
    "      momentum: {type: float, default: 0.5}\n",
    "      log_interval: {type: int, default: 10}\n",
    "      random_seed: {type: int, default: 1}\n",
    "    command: 'python main.py --batch_size_train={batch_size_train} --batch_size_test={batch_size_test}\n",
    "                                      --n_epochs={n_epochs} --learning_rate={learning_rate} --momentum={momentum}\n",
    "                                      --log_interval={log_interval} --random_seed={random_seed}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import mlflow\n",
    "import click\n",
    "\n",
    "from net import Net\n",
    "\n",
    "\n",
    "def load_data(batch_size_train, batch_size_test):\n",
    "    tmpdir = tempfile.mkdtemp()\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            tmpdir, train=True, download=True,\n",
    "            transform=torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(\n",
    "                    (0.1307,), (0.3081,))\n",
    "            ])\n",
    "        ),\n",
    "        batch_size=batch_size_train, shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            tmpdir, train=False, download=True,\n",
    "            transform=torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(\n",
    "                    (0.1307,), (0.3081,))\n",
    "            ])\n",
    "        ),\n",
    "        batch_size=batch_size_test, shuffle=True\n",
    "    )\n",
    "\n",
    "    mlflow.log_artifacts(f'{tmpdir}/MNIST', 'mnist_data')\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train(network, optimizer, train_loader, epoch, log_interval):\n",
    "    tmpdir = tempfile.mkdtemp()\n",
    "\n",
    "    network.train()\n",
    "    for batch, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % log_interval == 0:\n",
    "            mlflow.log_metric(f'Training Loss - Epoch {epoch}', loss.item(), batch * len(data))\n",
    "\n",
    "            n_complete = batch * len(data)\n",
    "            n_total = len(train_loader.dataset)\n",
    "            pct_complete = 100. * batch / len(train_loader)\n",
    "            print(f'Train Epoch: {epoch} [{n_complete}/{n_total} ({pct_complete:.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "            torch.save(network.state_dict(), f'{tmpdir}/model.pth')\n",
    "            torch.save(optimizer.state_dict(), f'{tmpdir}/optimizer.pth')\n",
    "\n",
    "    mlflow.log_artifacts(tmpdir, 'model_state')\n",
    "\n",
    "\n",
    "def test(network, test_loader, epoch):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    n_total = len(test_loader.dataset)\n",
    "    test_loss /= n_total\n",
    "    pct_correct = 100. * correct / n_total\n",
    "\n",
    "    mlflow.log_metric('Test Accuracy', pct_correct.item(), epoch)\n",
    "    mlflow.log_metric('Average Test Loss', test_loss, epoch)\n",
    "    print(f'\\nTest set: Avg. loss: {test_loss:.4f}, Accuracy: {correct}/{n_total} ({pct_correct:.0f}%)\\n')\n",
    "\n",
    "\n",
    "@click.command('Trains a neural network to classify images from the MNIST data set')\n",
    "@click.option('--batch_size_train', default=64)\n",
    "@click.option('--batch_size_test', default=10000)\n",
    "@click.option('--n_epochs', default=3)\n",
    "@click.option('--learning_rate', default=0.01)\n",
    "@click.option('--momentum', default=0.5)\n",
    "@click.option('--log_interval', default=10)\n",
    "@click.option('--random_seed', default=1)\n",
    "def train_network(batch_size_train, batch_size_test, n_epochs, learning_rate, momentum, log_interval, random_seed):\n",
    "    with mlflow.start_run():\n",
    "        train_loader, test_loader = load_data(batch_size_train, batch_size_test)\n",
    "\n",
    "        torch.backends.cudnn.enabled = False\n",
    "        torch.manual_seed(random_seed)\n",
    "\n",
    "        network = Net()\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        test(network, test_loader, 0)\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train(network, optimizer, train_loader, epoch, log_interval)\n",
    "            test(network, test_loader, epoch)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`net.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # noinspection PyTypeChecker\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using MLflow\n",
    "\n",
    "Now we can finally begin working with MLflow. You first need to create an experiment which will be used to organize the runs of your project. Navigate to the `03 - MLflow` directory in your terminal, and run the following to create a new experiment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mlflow experiments create --experiment-name mnist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, you can initialize a local server to host an interactive, web-based UI for monitoring and viewing results from your experiment runs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mlflow ui"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, you can go ahead and run your experiment! The command below will do so using default values for all parameters defined in the `MLproject` file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mlflow run . --no-conda"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to run your experiment with non-default parameters values, you can include the `-P` flag and parameter name and value pairs. Try re-running your experiment, changing the values of one or more parameters defined in the `MLproject` file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mlflow run . -P <PARAMETER_NAME>=<PARAMETER_VALUE> --no-conda"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}